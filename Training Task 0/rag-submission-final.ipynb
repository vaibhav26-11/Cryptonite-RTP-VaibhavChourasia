{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14857636,"sourceType":"datasetVersion","datasetId":9503962}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# FINAL RAG Wikipedia Answerer (Best Stable Version)\n# Fully Spec Compliant | Strong Grounding | Deterministic\n# =============================================================================\n\nimport re\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# ---------------- CONFIG ----------------\n\nCHUNK_TOKENS = 400\nOVERLAP = 80\nTOP_K = 3\nTOP_SENTENCES = 5\nTHRESHOLD = 0.60\nMAX_CHARS = 50_000_000\nBATCH_SIZE = 32\nMAX_PROMPT_TOKENS = 460\n\nDATA_PATH = \"/kaggle/input/datasets/vaibhavchourasia2611/wikipedia-english/AllCombined.txt\"\nEMBED_MODEL = \"intfloat/e5-large\"\nLLM_MODEL = \"google/flan-t5-base\"\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Device:\", DEVICE)\n\n# ---------------- LOAD DATA ----------------\n\nprint(\"Loading dataset...\")\nwith open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n    raw_text = f.read(MAX_CHARS)\nprint(f\"Loaded {len(raw_text):,} chars\")\n\n# ---------------- ARTICLE SPLIT ----------------\n\narticles = []\ncurrent_title = None\ncurrent_text = []\n\nfor line in raw_text.split(\"\\n\"):\n    line = line.strip()\n    if not line:\n        continue\n    if len(line) < 80 and not line.endswith(\".\") and not line.endswith(\",\"):\n        if current_title and current_text:\n            articles.append((current_title, \" \".join(current_text)))\n        current_title = line\n        current_text = []\n    else:\n        current_text.append(line)\n\nif current_title and current_text:\n    articles.append((current_title, \" \".join(current_text)))\n\nprint(f\"Articles: {len(articles)}\")\n\n# ---------------- TOKENIZERS ----------------\n\nembed_tokenizer = AutoTokenizer.from_pretrained(EMBED_MODEL)\nllm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n\n# ---------------- CHUNKING ----------------\n\nchunks = []\nchunk_titles = []\nstride = CHUNK_TOKENS - OVERLAP\n\nfor title, text in articles:\n    paras = [p.strip() for p in re.split(r\"\\n+\", text) if len(p.strip()) > 20]\n    ids = []\n    for p in paras:\n        ids.extend(embed_tokenizer.encode(\n            p, add_special_tokens=False, max_length=512, truncation=True))\n    for i in range(0, len(ids), stride):\n        piece = ids[i:i+CHUNK_TOKENS]\n        if len(piece) >= 80:\n            chunks.append(embed_tokenizer.decode(piece, skip_special_tokens=True))\n            chunk_titles.append(title.lower())\n\nprint(f\"Chunks: {len(chunks)}\")\n\n# ---------------- EMBEDDING ----------------\n\nembed_model = AutoModel.from_pretrained(EMBED_MODEL).to(DEVICE)\nembed_model.eval()\n\ndef encode(texts, is_query=False):\n    prefix = \"query: \" if is_query else \"passage: \"\n    prefixed = [prefix + t for t in texts]\n    enc = embed_tokenizer(prefixed, return_tensors=\"pt\",\n                          truncation=True, max_length=192, padding=True)\n    enc = {k: v.to(DEVICE) for k, v in enc.items()}\n    with torch.no_grad():\n        hidden = embed_model(**enc).last_hidden_state\n    mask = enc[\"attention_mask\"].unsqueeze(-1).float()\n    pooled = (hidden * mask).sum(1) / mask.sum(1)\n    vecs = pooled.cpu().numpy().astype(\"float32\")\n    return vecs / (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-9)\n\nprint(\"Building index...\")\nemb_list = []\nfor i in range(0, len(chunks), BATCH_SIZE):\n    emb_list.append(encode(chunks[i:i+BATCH_SIZE], False))\nchunk_embeddings = np.vstack(emb_list)\nprint(\"Index shape:\", chunk_embeddings.shape)\n\n# ---------------- LLM ----------------\n\nllm = AutoModelForSeq2SeqLM.from_pretrained(LLM_MODEL).to(DEVICE)\nllm.eval()\n\n# ---------------- RETRIEVE ----------------\n\ndef retrieve(question):\n    q_vec = encode([question], True)\n    scores = cosine_similarity(q_vec, chunk_embeddings)[0]\n\n    q_lower = question.lower()\n    boosted = scores.copy()\n\n    for i, title in enumerate(chunk_titles):\n        if title and title in q_lower:\n            boosted[i] += 0.25\n\n    idx = np.argsort(boosted)[::-1][:TOP_K]\n    return [chunks[i] for i in idx], scores[idx], round(float(max(scores)),2)\n\n# ---------------- REFUSAL ----------------\n\ndef should_refuse(question, raw_scores, top_chunks):\n    if max(raw_scores) < THRESHOLD:\n        return True\n\n    q_clean = re.sub(r\"^(who|what|where|when|how|why|is|was|are|were)\\s+\",\n                     \"\", question, flags=re.IGNORECASE).strip()\n    proper = re.findall(r\"\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\\b\", q_clean)\n\n    if proper:\n        combined = \" \".join(top_chunks)\n        for name in proper:\n            if re.search(re.escape(name), combined, re.IGNORECASE):\n                return False\n            surname = name.split()[-1]\n            if re.search(r\"\\b\" + re.escape(surname) + r\"\\b\", combined, re.IGNORECASE):\n                return False\n        return True\n\n    return False\n\n# ---------------- SENTENCE FILTERING ----------------\n\nFINITE_V = re.compile(\n    r\"\\b(is|are|was|were|has|have|had|invented|discovered|born|died|created|\"\n    r\"made|wrote|built|refers|known|called|converts|uses|became|worked|won|directed)\\b\",\n    re.IGNORECASE)\n\ndef is_good(s):\n    if len(s.split()) < 6: return False\n    if not s[0].isupper(): return False\n    if not FINITE_V.search(s): return False\n    if re.search(r\"formula_\\d\", s): return False\n    return True\n\ndef select_sentences(question, top_chunks):\n    sentences = []\n    for chunk in top_chunks:\n        for s in re.split(r\"(?<=[.!?])\\s+\", chunk):\n            s = s.strip()\n            if s and is_good(s):\n                sentences.append(s)\n\n    if not sentences:\n        return \" \".join(top_chunks)\n\n    q_vec = encode([question], True)\n    s_vecs = encode(sentences, False)\n    scores = cosine_similarity(q_vec, s_vecs)[0]\n\n    idx = np.argsort(scores)[::-1][:TOP_SENTENCES]\n    return \" \".join([sentences[i] for i in idx])\n\n# ---------------- GENERATE ----------------\n\ndef generate(question, context):\n    prefix = (\n        \"Answer the question in 2 to 3 complete sentences.\\n\"\n        \"Use only the facts written in the context.\\n\"\n        \"Do not add any new information.\\n\"\n        \"Rewrite in simple English.\\n\\n\"\n        \"Context:\\n\"\n    )\n    suffix = f\"\\nQuestion: {question}\\nAnswer:\"\n    overhead = len(llm_tokenizer.encode(prefix + suffix))\n    budget = MAX_PROMPT_TOKENS - overhead\n\n    ctx_ids = llm_tokenizer.convert_tokens_to_ids(llm_tokenizer.tokenize(context))\n    if len(ctx_ids) > budget:\n        context = llm_tokenizer.decode(ctx_ids[:budget], skip_special_tokens=True)\n\n    prompt = prefix + context + suffix\n\n    inputs = llm_tokenizer(prompt, return_tensors=\"pt\",\n                           truncation=True,\n                           max_length=MAX_PROMPT_TOKENS).to(DEVICE)\n\n    with torch.no_grad():\n        out = llm.generate(\n            **inputs,\n            min_new_tokens=80,\n            max_new_tokens=120,\n            do_sample=False,\n            num_beams=4,\n            no_repeat_ngram_size=3,\n            length_penalty=1.5,\n            early_stopping=True\n        )\n\n    return llm_tokenizer.decode(out[0], skip_special_tokens=True)\n\n# ---------------- POST PROCESS ----------------\n\ndef post_process(text):\n    text = re.sub(r\"\\([^)]*\\)\", \"\", text)\n    text = re.sub(r\"\\[[^\\]]*\\]\", \"\", text)\n    text = re.sub(r\"formula_\\d+\", \"\", text)\n    text = re.sub(r\"\\s-\\s\", \"-\", text)\n    text = re.sub(r\"\\s+,\", \",\", text)\n    text = re.sub(r\"\\s+\\.\", \".\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n\n    sents = [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n    result = \" \".join(sents[:3])\n\n    if result and result[-1] not in \".!?\":\n        result += \".\"\n    return result\n\n# ---------------- FULL PIPELINE ----------------\n\ndef answer_question(question):\n    top_chunks, raw_scores, confidence = retrieve(question)\n\n    print(\"\\nQuestion :\", question)\n    print(\"Confidence:\", confidence)\n\n    if should_refuse(question, raw_scores, top_chunks):\n        print(\"Answer    : Not enough information in the Simple Wikipedia dataset.\")\n        return\n\n    context = select_sentences(question, top_chunks)\n    raw = generate(question, context)\n    answer = post_process(raw)\n\n    if not answer:\n        answer = \"Not enough information in the Simple Wikipedia dataset.\"\n\n    print(\"Answer    :\", answer)\n\n# ---------------- TEST ----------------\n\nprint(\"=\"*60)\nprint(f\"embed={EMBED_MODEL} | llm={LLM_MODEL}\")\nprint(\"=\"*60)\n\nfor q in [\n    \"What is photosynthesis?\",\n    \"Who was Albert Einstein?\",\n    \"How does gravity work?\",\n    \"Who invented the telephone?\",\n]:\n    answer_question(q)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T06:46:23.757352Z","iopub.execute_input":"2026-02-17T06:46:23.757573Z","iopub.status.idle":"2026-02-17T07:04:46.016898Z","shell.execute_reply.started":"2026-02-17T06:46:23.757551Z","shell.execute_reply":"2026-02-17T07:04:46.016096Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\nLoading dataset...\nLoaded 50,000,000 chars\nArticles: 47817\nChunks: 32409\n","output_type":"stream"},{"name":"stderr","text":"2026-02-17 06:47:15.911668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771310835.934225     191 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771310835.940755     191 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771310835.957234     191 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771310835.957251     191 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771310835.957254     191 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771310835.957256     191 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Building index...\nIndex shape: (32409, 1024)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b279954fb74046508e4b8dfbea033dae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcee88e8f9a34d3e969e6aab2db17b4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4434db2b02b471b848b8e4882ce3f69"}},"metadata":{}},{"name":"stdout","text":"============================================================\nembed=intfloat/e5-large | llm=google/flan-t5-base\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (770 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"\nQuestion : What is photosynthesis?\nConfidence: 0.9\nAnswer    : Photosynthesis is an endothermic chemical process which uses sunlight to turn carbon dioxide into sugars. The sugars are used by the cell as energy, and to build other kinds of molecules. fundamentally, photosynthesis converts light energy into chemical energy.\n\nQuestion : Who was Albert Einstein?\nConfidence: 0.88\nAnswer    : he developed the theory of relativity. he won the nobel prize in physics in 1921 for theoretical physics. his most famous equation is formula _ 1 in which e is for energy, m for mass, c is the speed of light is therefore \" energy \" equals \" mass \" multiplied by \" the speed _ of light \" squared.\n\nQuestion : How does gravity work?\nConfidence: 0.89\nAnswer    : gravity, or gravitation, is one of the fundamental forces of the universe. it is an attraction, or pull, between any two objects with mass. we discuss it in three parts : some physicists think gravity is caused by gravitons, but they are still unsure.\n\nQuestion : Who invented the telephone?\nConfidence: 0.89\nAnswer    : alexander graham bell was the first person to patent the telephone, in 1876. early telephones were wired directly to each other and could only talk to the phone that they were connected to. later, telephone exchanges allowed connecting to other telephones.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}